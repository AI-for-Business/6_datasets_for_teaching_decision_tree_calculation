General information:
	|S| = 25
	remaining columns: ['do sports', 'weight', 'stress level', 'age', 'diet', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		low risk for heart attack: 17
		high risk for heart attack: 8
	Calculate the entropy:
		Entropy(S) = (17/25) * log_2(17/25) + (8/25) * log_2(8/25) = 0.904
Calculate the information gain of all attributes:
	do sports:
		Calculate the entropy of all values of the attribute:
			regularly:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 5
					high risk for heart attack: 2
				Calculate the entropy:
					Entropy(S_regularly) = (5/7) * log_2(5/7) + (2/7) * log_2(2/7) = 0.863
			irregularly:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 3
					low risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_irregularly) = (3/6) * log_2(3/6) + (3/6) * log_2(3/6) = 1.0
			not at all:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 9
					high risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_not at all) = (9/12) * log_2(9/12) + (3/12) * log_2(3/12) = 0.811
		Calculate the information gain for the attribute:
			Gain(S,do sports) = (7/25) * Entropy(S_regularly) + (6/25) * Entropy(S_irregularly) + (12/25) * Entropy(S_not at all) = 0.033
	weight:
		Calculate the entropy of all values of the attribute:
			normal weight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 8
					high risk for heart attack: 5
				Calculate the entropy:
					Entropy(S_normal weight) = (8/13) * log_2(8/13) + (5/13) * log_2(5/13) = 0.961
			overweight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 7
					high risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_overweight) = (7/10) * log_2(7/10) + (3/10) * log_2(3/10) = 0.881
			underweight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 2
				Calculate the entropy:
					Entropy(S_underweight) = (2/2) * log_2(2/2) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,weight) = (13/25) * Entropy(S_normal weight) + (10/25) * Entropy(S_overweight) + (2/25) * Entropy(S_underweight) = 0.052
	stress level:
		Calculate the entropy of all values of the attribute:
			high:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 7
					low risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_high) = (7/10) * log_2(7/10) + (3/10) * log_2(3/10) = 0.881
			medium:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 9
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_medium) = (9/10) * log_2(9/10) + (1/10) * log_2(1/10) = 0.469
			low:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 5
				Calculate the entropy:
					Entropy(S_low) = (5/5) * log_2(5/5) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,stress level) = (10/25) * Entropy(S_high) + (10/25) * Entropy(S_medium) + (5/25) * Entropy(S_low) = 0.364
	age:
		Calculate the entropy of all values of the attribute:
			30-50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 5
					high risk for heart attack: 4
				Calculate the entropy:
					Entropy(S_30-50) = (5/9) * log_2(5/9) + (4/9) * log_2(4/9) = 0.991
			<30:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 7
					high risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_<30) = (7/10) * log_2(7/10) + (3/10) * log_2(3/10) = 0.881
			>50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 5
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_>50) = (5/6) * log_2(5/6) + (1/6) * log_2(1/6) = 0.65
		Calculate the information gain for the attribute:
			Gain(S,age) = (9/25) * Entropy(S_30-50) + (10/25) * Entropy(S_<30) + (6/25) * Entropy(S_>50) = 0.039
	diet:
		Calculate the entropy of all values of the attribute:
			unhealthy:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 6
					low risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_unhealthy) = (6/9) * log_2(6/9) + (3/9) * log_2(3/9) = 0.918
			healthy:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 9
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_healthy) = (9/10) * log_2(9/10) + (1/10) * log_2(1/10) = 0.469
			neutral:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 5
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_neutral) = (5/6) * log_2(5/6) + (1/6) * log_2(1/6) = 0.65
		Calculate the information gain for the attribute:
			Gain(S,diet) = (9/25) * Entropy(S_unhealthy) + (10/25) * Entropy(S_healthy) + (6/25) * Entropy(S_neutral) = 0.23
Determine the best attribute for splitting: 
	max{Gain(S,do sports), Gain(S,weight), Gain(S,stress level), Gain(S,age), Gain(S,diet)} = Gain(S,stress level) --> split at stress level
Create the subtree:
	Create the node stress level
	Create a child node for every value of stress level:
		high:
			There is more than one target attribute value left (i. e. we have no perfect entropy) and we can perform an additional split.
			Split at the attribute which leads to the highest information gain. --> Create diet as the child node.
			Create an edge from stress level to diet with the label high.
		medium:
			There is more than one target attribute value left (i. e. we have no perfect entropy) and we can perform an additional split.
			Split at the attribute which leads to the highest information gain. --> Create age as the child node.
			Create an edge from stress level to age with the label medium.
		low:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from stress level to low risk for heart attack with the label low.


This is the approach for the creation of the subtree with diet as the root.
General information:
	|S| = 10
	remaining columns: ['do sports', 'weight', 'age', 'diet', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		high risk for heart attack: 7
		low risk for heart attack: 3
	Calculate the entropy:
		Entropy(S) = (7/10) * log_2(7/10) + (3/10) * log_2(3/10) = 0.881
Calculate the information gain of all attributes:
	do sports:
		Calculate the entropy of all values of the attribute:
			regularly:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 2
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_regularly) = (2/3) * log_2(2/3) + (1/3) * log_2(1/3) = 0.918
			irregularly:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 3
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_irregularly) = (3/4) * log_2(3/4) + (1/4) * log_2(1/4) = 0.811
			not at all:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 2
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_not at all) = (2/3) * log_2(2/3) + (1/3) * log_2(1/3) = 0.918
		Calculate the information gain for the attribute:
			Gain(S,do sports) = (3/10) * Entropy(S_regularly) + (4/10) * Entropy(S_irregularly) + (3/10) * Entropy(S_not at all) = 0.006
	weight:
		Calculate the entropy of all values of the attribute:
			normal weight:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 5
					low risk for heart attack: 2
				Calculate the entropy:
					Entropy(S_normal weight) = (5/7) * log_2(5/7) + (2/7) * log_2(2/7) = 0.863
			overweight:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 2
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_overweight) = (2/3) * log_2(2/3) + (1/3) * log_2(1/3) = 0.918
		Calculate the information gain for the attribute:
			Gain(S,weight) = (7/10) * Entropy(S_normal weight) + (3/10) * Entropy(S_overweight) = 0.002
	age:
		Calculate the entropy of all values of the attribute:
			30-50:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 4
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_30-50) = (4/5) * log_2(4/5) + (1/5) * log_2(1/5) = 0.722
			<30:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 3
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_<30) = (3/4) * log_2(3/4) + (1/4) * log_2(1/4) = 0.811
			>50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_>50) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,age) = (5/10) * Entropy(S_30-50) + (4/10) * Entropy(S_<30) + (1/10) * Entropy(S_>50) = 0.196
	diet:
		Calculate the entropy of all values of the attribute:
			unhealthy:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 5
				Calculate the entropy:
					Entropy(S_unhealthy) = (5/5) * log_2(5/5) = 0.0
			healthy:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 2
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_healthy) = (2/3) * log_2(2/3) + (1/3) * log_2(1/3) = 0.918
			neutral:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_neutral) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
		Calculate the information gain for the attribute:
			Gain(S,diet) = (5/10) * Entropy(S_unhealthy) + (3/10) * Entropy(S_healthy) + (2/10) * Entropy(S_neutral) = 0.406
Determine the best attribute for splitting: 
	max{Gain(S,do sports), Gain(S,weight), Gain(S,age), Gain(S,diet)} = Gain(S,diet) --> split at diet
Create the subtree:
	Create the node diet
	Create a child node for every value of diet:
		unhealthy:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create high risk for heart attack as the child node.
			Create an edge from diet to high risk for heart attack with the label unhealthy.
		healthy:
			There is more than one target attribute value left (i. e. we have no perfect entropy) and we can perform an additional split.
			Split at the attribute which leads to the highest information gain. --> Create do sports as the child node.
			Create an edge from diet to do sports with the label healthy.
		neutral:
			There is more than one target attribute value left (i. e. we have no perfect entropy) and we can perform an additional split.
			Split at the attribute which leads to the highest information gain. --> Create do sports as the child node.
			Create an edge from diet to do sports with the label neutral.


This is the approach for the creation of the subtree with do sports as the root.
General information:
	|S| = 3
	remaining columns: ['do sports', 'weight', 'age', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		low risk for heart attack: 2
		high risk for heart attack: 1
	Calculate the entropy:
		Entropy(S) = (2/3) * log_2(2/3) + (1/3) * log_2(1/3) = 0.918
Calculate the information gain of all attributes:
	do sports:
		Calculate the entropy of all values of the attribute:
			not at all:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_not at all) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
			irregularly:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_irregularly) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,do sports) = (2/3) * Entropy(S_not at all) + (1/3) * Entropy(S_irregularly) = 0.252
	weight:
		Calculate the entropy of all values of the attribute:
			normal weight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_normal weight) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
			overweight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_overweight) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,weight) = (2/3) * Entropy(S_normal weight) + (1/3) * Entropy(S_overweight) = 0.252
	age:
		Calculate the entropy of all values of the attribute:
			>50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_>50) = (1/1) * log_2(1/1) = 0.0
			30-50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_30-50) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
		Calculate the information gain for the attribute:
			Gain(S,age) = (1/3) * Entropy(S_>50) + (2/3) * Entropy(S_30-50) = 0.252
Determine the best attribute for splitting: 
	max{Gain(S,do sports), Gain(S,weight), Gain(S,age)} = Gain(S,do sports) --> split at do sports
Create the subtree:
	Create the node do sports
	Create a child node for every value of do sports:
		not at all:
			There is more than one target attribute value left (i. e. we have no perfect entropy) and we can perform an additional split.
			Split at the attribute which leads to the highest information gain. --> Create age as the child node.
			Create an edge from do sports to age with the label not at all.
		irregularly:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from do sports to low risk for heart attack with the label irregularly.


This is the approach for the creation of the subtree with age as the root.
General information:
	|S| = 2
	remaining columns: ['weight', 'age', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		low risk for heart attack: 1
		high risk for heart attack: 1
	Calculate the entropy:
		Entropy(S) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
Calculate the information gain of all attributes:
	weight:
		Calculate the entropy of all values of the attribute:
			normal weight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_normal weight) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
		Calculate the information gain for the attribute:
			Gain(S,weight) = (2/2) * Entropy(S_normal weight) = 0.0
	age:
		Calculate the entropy of all values of the attribute:
			>50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_>50) = (1/1) * log_2(1/1) = 0.0
			30-50:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_30-50) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,age) = (1/2) * Entropy(S_>50) + (1/2) * Entropy(S_30-50) = 1.0
Determine the best attribute for splitting: 
	max{Gain(S,weight), Gain(S,age)} = Gain(S,age) --> split at age
Create the subtree:
	Create the node age
	Create a child node for every value of age:
		>50:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from age to low risk for heart attack with the label >50.
		30-50:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create high risk for heart attack as the child node.
			Create an edge from age to high risk for heart attack with the label 30-50.


This is the approach for the creation of the subtree with do sports as the root.
General information:
	|S| = 2
	remaining columns: ['do sports', 'weight', 'age', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		low risk for heart attack: 1
		high risk for heart attack: 1
	Calculate the entropy:
		Entropy(S) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
Calculate the information gain of all attributes:
	do sports:
		Calculate the entropy of all values of the attribute:
			regularly:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_regularly) = (1/1) * log_2(1/1) = 0.0
			irregularly:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_irregularly) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,do sports) = (1/2) * Entropy(S_regularly) + (1/2) * Entropy(S_irregularly) = 1.0
	weight:
		Calculate the entropy of all values of the attribute:
			normal weight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_normal weight) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
		Calculate the information gain for the attribute:
			Gain(S,weight) = (2/2) * Entropy(S_normal weight) = 0.0
	age:
		Calculate the entropy of all values of the attribute:
			<30:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_<30) = (1/1) * log_2(1/1) = 0.0
			30-50:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_30-50) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,age) = (1/2) * Entropy(S_<30) + (1/2) * Entropy(S_30-50) = 1.0
Determine the best attribute for splitting: 
	max{Gain(S,do sports), Gain(S,weight), Gain(S,age)} = Gain(S,do sports) --> split at do sports
Create the subtree:
	Create the node do sports
	Create a child node for every value of do sports:
		regularly:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from do sports to low risk for heart attack with the label regularly.
		irregularly:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create high risk for heart attack as the child node.
			Create an edge from do sports to high risk for heart attack with the label irregularly.


This is the approach for the creation of the subtree with age as the root.
General information:
	|S| = 10
	remaining columns: ['do sports', 'weight', 'age', 'diet', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		low risk for heart attack: 9
		high risk for heart attack: 1
	Calculate the entropy:
		Entropy(S) = (9/10) * log_2(9/10) + (1/10) * log_2(1/10) = 0.469
Calculate the information gain of all attributes:
	do sports:
		Calculate the entropy of all values of the attribute:
			regularly:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 2
				Calculate the entropy:
					Entropy(S_regularly) = (2/2) * log_2(2/2) = 0.0
			irregularly:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_irregularly) = (1/1) * log_2(1/1) = 0.0
			not at all:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 6
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_not at all) = (6/7) * log_2(6/7) + (1/7) * log_2(1/7) = 0.592
		Calculate the information gain for the attribute:
			Gain(S,do sports) = (2/10) * Entropy(S_regularly) + (1/10) * Entropy(S_irregularly) + (7/10) * Entropy(S_not at all) = 0.055
	weight:
		Calculate the entropy of all values of the attribute:
			overweight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 4
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_overweight) = (4/5) * log_2(4/5) + (1/5) * log_2(1/5) = 0.722
			underweight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 2
				Calculate the entropy:
					Entropy(S_underweight) = (2/2) * log_2(2/2) = 0.0
			normal weight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 3
				Calculate the entropy:
					Entropy(S_normal weight) = (3/3) * log_2(3/3) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,weight) = (5/10) * Entropy(S_overweight) + (2/10) * Entropy(S_underweight) + (3/10) * Entropy(S_normal weight) = 0.108
	age:
		Calculate the entropy of all values of the attribute:
			>50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_>50) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
			30-50:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 4
				Calculate the entropy:
					Entropy(S_30-50) = (4/4) * log_2(4/4) = 0.0
			<30:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 4
				Calculate the entropy:
					Entropy(S_<30) = (4/4) * log_2(4/4) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,age) = (2/10) * Entropy(S_>50) + (4/10) * Entropy(S_30-50) + (4/10) * Entropy(S_<30) = 0.269
	diet:
		Calculate the entropy of all values of the attribute:
			healthy:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 4
				Calculate the entropy:
					Entropy(S_healthy) = (4/4) * log_2(4/4) = 0.0
			unhealthy:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 3
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_unhealthy) = (3/4) * log_2(3/4) + (1/4) * log_2(1/4) = 0.811
			neutral:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 2
				Calculate the entropy:
					Entropy(S_neutral) = (2/2) * log_2(2/2) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,diet) = (4/10) * Entropy(S_healthy) + (4/10) * Entropy(S_unhealthy) + (2/10) * Entropy(S_neutral) = 0.144
Determine the best attribute for splitting: 
	max{Gain(S,do sports), Gain(S,weight), Gain(S,age), Gain(S,diet)} = Gain(S,age) --> split at age
Create the subtree:
	Create the node age
	Create a child node for every value of age:
		>50:
			There is more than one target attribute value left (i. e. we have no perfect entropy) and we can perform an additional split.
			Split at the attribute which leads to the highest information gain. --> Create do sports as the child node.
			Create an edge from age to do sports with the label >50.
		30-50:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from age to low risk for heart attack with the label 30-50.
		<30:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from age to low risk for heart attack with the label <30.


This is the approach for the creation of the subtree with do sports as the root.
General information:
	|S| = 2
	remaining columns: ['do sports', 'weight', 'diet', 'risk for heart attack']
Calculate the entropy of the subset:
	Count the occurrence of each target attribute value:
		low risk for heart attack: 1
		high risk for heart attack: 1
	Calculate the entropy:
		Entropy(S) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
Calculate the information gain of all attributes:
	do sports:
		Calculate the entropy of all values of the attribute:
			regularly:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_regularly) = (1/1) * log_2(1/1) = 0.0
			not at all:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_not at all) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,do sports) = (1/2) * Entropy(S_regularly) + (1/2) * Entropy(S_not at all) = 1.0
	weight:
		Calculate the entropy of all values of the attribute:
			overweight:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_overweight) = (1/2) * log_2(1/2) + (1/2) * log_2(1/2) = 1.0
		Calculate the information gain for the attribute:
			Gain(S,weight) = (2/2) * Entropy(S_overweight) = 0.0
	diet:
		Calculate the entropy of all values of the attribute:
			healthy:
				Count the occurrence of each target attribute value:
					low risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_healthy) = (1/1) * log_2(1/1) = 0.0
			unhealthy:
				Count the occurrence of each target attribute value:
					high risk for heart attack: 1
				Calculate the entropy:
					Entropy(S_unhealthy) = (1/1) * log_2(1/1) = 0.0
		Calculate the information gain for the attribute:
			Gain(S,diet) = (1/2) * Entropy(S_healthy) + (1/2) * Entropy(S_unhealthy) = 1.0
Determine the best attribute for splitting: 
	max{Gain(S,do sports), Gain(S,weight), Gain(S,diet)} = Gain(S,do sports) --> split at do sports
Create the subtree:
	Create the node do sports
	Create a child node for every value of do sports:
		regularly:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create low risk for heart attack as the child node.
			Create an edge from do sports to low risk for heart attack with the label regularly.
		not at all:
			There is only target attribute value left (i. e. we have perfect entropy). --> Create high risk for heart attack as the child node.
			Create an edge from do sports to high risk for heart attack with the label not at all.
